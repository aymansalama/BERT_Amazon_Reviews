{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Amazon_Reviews",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv0j5yFQ1Bmo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "dd263dbf-215e-42c2-f2ae-9330f8a2aaaa"
      },
      "source": [
        "! pip install pandas\n",
        "! pip install torch\n",
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 56.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 54.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 56.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=fabfc63537e18e6462165ec03c54d02d9d698c0a2294a5907c71ab0efc734274\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q17CaOhm6Q3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFT54wV52BDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import  BertModel, BertTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import dataLoader\n",
        "import classifier\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Hkph5m5s3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n3kKqRm2LcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_and_shuffle(file):\n",
        "    df = pd.read_csv(file, delimiter=',')\n",
        "    # Random shuffle.\n",
        "    df.sample(frac=1)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oerGzXhG2Qjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_and_val_split(df, splitRatio=0.8):\n",
        "    train=df.sample(frac=splitRatio,random_state=200)\n",
        "    val=df.drop(train.index)\n",
        "    print(\"Number of Training Samples: \", len(train))\n",
        "    print(\"Number of Validation Samples: \", len(val))\n",
        "    return(train, val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9naLmqgV2Sfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_max_length(reviews):\n",
        "    return len(max(reviews, key=len))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruXknayR2VDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(logits, labels):\n",
        "    # get the index of the max value in the row.\n",
        "    predictedClass = logits.max(dim = 1)[1]\n",
        "\n",
        "    # get accuracy by averaging over entire batch.\n",
        "    acc = (predictedClass == labels).float().mean()\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vBprhKJ2ZVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainFunc(net, loss_func, opti, train_loader, test_loader, config):\n",
        "    best_acc = 0\n",
        "    for ep in range(config[\"epochs\"]):\n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            opti.zero_grad()\n",
        "            #seq, attn_masks, labels = seq.cuda(args.gpu), attn_masks.cuda(args.gpu), labels.cuda(args.gpu)\n",
        "            seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)\n",
        "\n",
        "            logits = net(seq, attn_masks)\n",
        "            loss = loss_func(m(logits), labels)\n",
        "\n",
        "            loss.backward()\n",
        "            opti.step()\n",
        "            print(\"Iteration: \", it+1)\n",
        "\n",
        "            if (it + 1) % config[\"printEvery\"] == 0:\n",
        "                acc = get_accuracy(m(logits), labels)\n",
        "                if not os.path.exists(config[\"outputFolder\"]):\n",
        "                    os.makedirs(config[\"outputFolder\"])\n",
        "\n",
        "                # Since a single epoch could take well over hours, we regularly save the model even during evaluation of training accuracy.\n",
        "                torch.save(net.state_dict(), os.path.join(config[\"outputFolder\"], config[\"outputFileName\"]))\n",
        "                print(\"Iteration {} of epoch {} complete. Loss : {} Accuracy : {}\".format(it+1, ep+1, loss.item(), acc))\n",
        "\n",
        "        # perform validation at the end of an epoch.\n",
        "        val_acc, val_loss = evaluate(net, loss_func, val_loader, config)\n",
        "        print(\" Validation Accuracy : {}, Validation Loss : {}\".format(val_acc, val_loss))\n",
        "        if val_acc > best_acc:\n",
        "            print(\"Best validation accuracy improved from {} to {}, saving model...\".format(best_acc, val_acc))\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), os.path.join(config[\"outputFolder\"], config[\"outputFileName\"] + \"_valTested_\" + str(best_acc)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSIptpqP2fRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(net, loss_func, dataloader, config):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, labels in dataloader:\n",
        "            #seq, attn_masks, labels = seq.cuda(args.gpu), attn_masks.cuda(args.gpu), labels.cuda(args.gpu)\n",
        "            seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)\n",
        "\n",
        "            logits = net(seq, attn_masks)\n",
        "            mean_loss += loss_func(m(logits), labels)\n",
        "            mean_acc += get_accuracy(m(logits), labels)\n",
        "            print(\"Validation iteration\", count+1)\n",
        "            count += 1\n",
        "\n",
        "            '''\n",
        "            The entire validation set was around 0.1 million entries,\n",
        "            the validationFraction param controls what fraction of the shuffled\n",
        "            validation set you want to validate the results on.\n",
        "            '''\n",
        "            if count > config[\"validationFraction\"] * len(val_set):\n",
        "                break\n",
        "    return mean_acc / count, mean_loss / count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljqzMJFn2mV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    \"splitRatio\" : 0.8,\n",
        "    \"maxLength\" : 100,\n",
        "    \"printEvery\" : 5,\n",
        "    \"outputFolder\" : \"Models\",\n",
        "    \"outputFileName\" : \"AmazonReviewClassifier.dat\",\n",
        "    \"threads\" : 4,\n",
        "    \"batchSize\" : 64,\n",
        "    \"validationFraction\" : 0.0005,\n",
        "    \"epochs\" : 5,\n",
        "    \"forceCPU\" : False\n",
        "    }\n",
        "if config[\"forceCPU\"]:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "config[\"device\"] = device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhyAAjcB2xqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, device, freeze_bert = True):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.device = device\n",
        "\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.cls_layer = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "\n",
        "        #Obtaining the representation of [CLS] head\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        #Feeding cls_rep to the classifier layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits.to(self.device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znUMPleM24mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AmazonReviewsDataset(Dataset):\n",
        "    def __init__(self, df, maxlen):\n",
        "        self.df = df\n",
        "        # A reset reindexes from 1 to len(df), the shuffled df frames are sparse.\n",
        "        self.df.reset_index(drop=True, inplace=True)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return(len(self.df))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        review = self.df.loc[index, 'Text']\n",
        "\n",
        "        # Classes start from 0.\n",
        "        label = int(self.df.loc[index, 'Score']) - 1\n",
        "\n",
        "        # Use BERT tokenizer since it needs to be able to match the tokens to the pre trained words.\n",
        "        tokens = self.tokenizer.tokenize(review)\n",
        "\n",
        "        # BERT inputs typically start with a '[CLS]' tag and end with a '[SEP]' tag. For\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "\n",
        "        if len(tokens) < self.maxlen:\n",
        "            # Add the ['PAD'] token\n",
        "            tokens = tokens + ['[PAD]' for item in range(self.maxlen-len(tokens))]\n",
        "        else:\n",
        "            # Truncate the tokens at maxLen - 1 and add a '[SEP]' tag.\n",
        "            tokens = tokens[:self.maxlen-1] + ['[SEP]']\n",
        "\n",
        "        # BERT tokenizer converts the string tokens to their respective IDs.\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # Converting to pytorch tensors.\n",
        "        tokens_ids_tensor = torch.tensor(token_ids)\n",
        "\n",
        "        # Masks place a 1 if token != PAD else a 0.\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "        \n",
        "        return tokens_ids_tensor, attn_mask, label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59pG-0ZW3FYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6qMODHX3MQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8cb9e7ba-3cea-4b5d-e55f-c95945321a2b"
      },
      "source": [
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "print(\"Configuration is: \", config)\n",
        "# Read and shuffle input data.\n",
        "df = read_and_shuffle(\"./drive/My Drive/Bert/AMAZON-DATASET/Reviews.csv\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n",
            "Configuration is:  {'splitRatio': 0.8, 'maxLength': 100, 'printEvery': 5, 'outputFolder': 'Models', 'outputFileName': 'AmazonReviewClassifier.dat', 'threads': 4, 'batchSize': 64, 'validationFraction': 0.0005, 'epochs': 5, 'forceCPU': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYyvxXRR3ceK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "263dc543-5d80-4d8e-b783-651dc0725fa8"
      },
      "source": [
        "num_classes = df['Score'].nunique()\n",
        "print(\"Number of Target Output Classes:\", num_classes)\n",
        "totalDatasetSize = len(df)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Target Output Classes: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P6nDjzD3hCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Group by the column Score. This helps you get distribution of the Review Scores.\n",
        "symbols = df.groupby('Score')\n",
        "\n",
        "scores_dist = []\n",
        "for i in range(num_classes):\n",
        "    scores_dist.append(len(symbols.groups[i+1])/totalDatasetSize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nmoAHec3mnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7b0f5704-00d9-453f-e7b8-e7c6e1830299"
      },
      "source": [
        "train, val = get_train_and_val_split(df, config[\"splitRatio\"])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Samples:  454763\n",
            "Number of Validation Samples:  113691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PQMMnpz3ptw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can set the length to the true max length from the dataset, I have reduced it for the sake of memory and quicker training.\n",
        "#T = get_max_length(reviews)\n",
        "T = config[\"maxLength\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA2kwsot3tkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = AmazonReviewsDataset(train, T)\n",
        "val_set = AmazonReviewsDataset(val, T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MbfDj7w31p-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_set, batch_size = config[\"batchSize\"], num_workers = config[\"threads\"])\n",
        "val_loader = DataLoader(val_set, batch_size = config[\"batchSize\"], num_workers = config[\"threads\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0gR6VMY39FG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We are unfreezing the BERT layers so as to be able to fine tune and save a new BERT model that is specific to the Sizeable food reviews dataset.\n",
        "\n",
        "net = SentimentClassifier(num_classes, config[\"device\"], freeze_bert=False)\n",
        "net.to(config[\"device\"])\n",
        "weights = torch.tensor(scores_dist).to(config[\"device\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP5uo4sf4MxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the Loss function and Optimizer.\n",
        "loss_func = nn.NLLLoss(weight=weights)\n",
        "opti = optim.Adam(net.parameters(), lr = 2e-5)\n",
        "m = nn.LogSoftmax(dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfvnHnNW4SRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "680a9b11-fa6a-48d3-8550-07bf8ee49231"
      },
      "source": [
        "torch.cuda.set_device(0)\n",
        "trainFunc(net, loss_func, opti, train_loader, val_loader, config)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  1\n",
            "Iteration:  2\n",
            "Iteration:  3\n",
            "Iteration:  4\n",
            "Iteration:  5\n",
            "Iteration 5 of epoch 1 complete. Loss : 0.42553701996803284 Accuracy : 0.609375\n",
            "Iteration:  6\n",
            "Iteration:  7\n",
            "Iteration:  8\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}